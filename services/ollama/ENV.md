# Ollama ENV â€” Contract

Ollama provides local LLM inference.

## Recommended
- `OLLAMA_MODEL` (e.g., `mistral:latest`)
- `OLLAMA_URL` (internal: `http://ollama:11434`)
